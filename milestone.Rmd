---
title: "Data Science Capstone Milestone Report"
author: "[Scott D. Weitzenhoffer](https://www.linkedin.com/in/sweitzen/)"
date: "February 11, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE)
```

## <a name="introduction"></a>Introduction

This report presents initial exploratory data analysis supporting the predictive
text model to be developed. The data for this project is the [Coursera Swiftkey 
Dataset](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip).
It consists of internet-derived text in English, Finnish, German, and Russian.
For this EDA, I will be using the English data, which consists of three files,
each of approximately 200Mb, collected from news sites, blogs, and twitter.
```{r}
zip_file <- "../Coursera-SwiftKey.zip"
pattern <- "en_US.*.txt"

# Create temp directory and extract files from zip archive
td <- tempdir()

# Get list of files to extract from the archive based on provided pattern
all_files <- unzip(zip_file, list=TRUE, exdir=td)
mask <- grepl(pattern, all_files$Name)
files_to_extract <- all_files[mask, ]

# Reformat Length column to be more meaningful
files_to_extract$Length <- round(files_to_extract$Length / 2^20, 4)
names(files_to_extract)[names(files_to_extract) == "Length"] <- "Size_Mb"

# Add a column for number of lines in file
files_to_extract$Num_Lines <- 0

# Extract the files
extracted_files <- unzip(zip_file, files_to_extract$Name, exdir=td)

# Initialize output
dat <- NULL
i <- 1

# Loop over extracted files
for(next_file in extracted_files) {
    # Read the next extracted file
    txt <- readLines(con=next_file, encoding="UTF-8", skipNul=TRUE)
    
    files_to_extract$Num_Lines[i] <- length(txt)
    
    # Concatenate data
    dat <- c(dat, txt)
    
    i <- i + 1
}

# Delete extracted files
unlink(extracted_files)

print(files_to_extract)

print(paste0(
    "Extracted data has ", length(dat), " lines and occupies ", 
    round(object.size(dat) / 2^20, 4), " Mb of memory"
))

rm(dat)
```

The R code supporting this report can be found on 
[GitHub](https://github.com/sweitzen/yap-tm). It will not be reproduced here.

---

## <a name="ingest"></a>Data Ingest
Once the data were read into memory, the lines were shuffled, and randomly
sampled, taking 98% for training, and 2% for testing. With such a large volume 
of data, 2%  is more than sufficient for testing.

The R package [quanteda](https://cran.r-project.org/web/packages/quanteda/index.html) 
was used to transform the raw data into tables of n-grams of size 1-5, and their
observed counts in the data. This process was very memory-intensive, and 
required splitting the data into chunks of 100,000 lines at a time. For each
chunk:

1. Construct a corpus of 100,000 lines of text.
2. Split corpus into individual sentences. This prevents n-grams from spanning 
sentences.
3. Split sentences up into n-gram tokens of sizes 1-5.
4. Construct a document-feature matrix (dfm) from the n-grams.
5. Using the dfm feature names and column sums, create a data.table from the 
ngrams with their frequency and count.

Converting n-gram tokens into document-feature matrices was the most taxing part 
of the process.

On my 8-year-old Dell Precision T3500 with a Xeon 3.33 GHz 6-core hyperthreaded
processor, 24 GB of RAM, and a WDC WD2002FAEX-007BA0 hard drive running under 
Kubuntu 16.04.3 linux, the runtimes were: 

* 02h:35m:42s to process the train data 
* 00h:11m:23s to process the test data 

The cleaning performed on the data was fairly light: replacing abbreviations, 
ordinals, symbols, converting to lowercase, and removing numbers, punctuation 
and whitespace. As I build and test the model, I may add or remove cleaning 
steps. 

---

## <a name="analysis"></a>Analysis
Analysis was fairly light, as the goal was not analysing the text itself for 
meaning, but simply determining frequent order of words. As such, I generated 
barplots of the most frequent combinations. Also included is the theoretical 
[Zipf frequency](https://en.wikipedia.org/wiki/Zipf%27s_law); as you can see, it 
holds best for 1-grams:
```{r}
library(data.table)
library(ggplot2)

makePlot <- function(data, title) {
    
    # total_count will be used to rescale count as frequency
    total_count <- sum(data$count)
    # max_freq will be used to generate theoretical Zipf frequency
    max_freq <- data[1, count] / total_count
    
    # Select top 30 data points
    data <- data[1:30]
    
    # Add columns for ngram, frequency and theoretical Zipf frequency
    for (i in 1:30) data[i, ':=' (
        ngram = gsub("_", " ", trimws(paste0(X, " ", y))),
        frequency = (count / total_count),
        zipf = max_freq / i
    )]
    
    ggplot(data=data) +
        geom_bar(
            mapping=aes(reorder(ngram, frequency), frequency, group=1),
            stat="identity",
            fill=I("darkred")
        ) +
        geom_line(
            mapping=aes(reorder(ngram, frequency), zipf, group=2),
            size=2,
            color="darkblue"
        ) +
        labs(
            x="Ngrams",
            y="Frequency"
        ) +
        ggtitle(title) +
        coord_flip()
}

load("../data/train/dts_pruned_8.rda")

# Maximum size of Ngrams
Nmax <- 5

# Order Ngrams by count (frequency)
for (j in 1:Nmax) {
    dts[[j]] <- dts[[j]][order(-count)]
}

# Plot barcharts of most common N-grams with theoretical Zipf frequency overlaid
makePlot(dts[[1]], "30 Most Common Unigrams")
makePlot(dts[[2]], "30 Most Common Bigrams")
makePlot(dts[[3]], "30 Most Common Trigrams")
makePlot(dts[[4]], "30 Most Common Quadgrams")
makePlot(dts[[5]], "30 Most Common Pentagrams")
```

Given that these data were collected from the internet, I was quite surprised
that the word "cat" does not appear in the top 30 unigrams. So, how far down the
list does it occur?
```{r}
match("cat", dts[[1]]$y)
```

Just for fun, let's plot a word cloud of common bigrams:
```{r}
library(wordcloud)

set.seed(222)

n <- 1

wordcloud(
    trimws(paste0(dts[[n]]$X, " ", dts[[n]]$y)),
    dts[[n]]$count, 
    scale=c(10,1),
    max.words=100,
    random.order=FALSE,
    rot.per=0.35,
    use.r.layout=FALSE,
    colors=brewer.pal(6, 'Spectral')
)
```

Since we're not performing sentiment analysis or trying to glean any deeper 
meaning from the words, other than in what order they tend to appear, I don't 
believe much more analysis at this point would be particularly fruitful.

---

## <a name="nextsteps"></a>Next Steps
A simple predictive text model can be constructed by splitting each n-gram 
(n = 2 to 5) into the first (n - 1) words (the input, X) and the last word (the 
prediction, y). User input can be matched against X, and a list of predictions y
can be returned in descending order of observed count from the training data. If
no predictions are found in n-grams (n > 2), then the 1-grams will at least 
return a list of most common words.

The test dataset will be split into X and y similarly, to compare against 
predictions made by the model.

When complete, this will be packaged into a Shiny app for evaluation.

---